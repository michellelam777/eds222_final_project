---
title: "Income Effects on SGIP Applications"
author: "Michelle Lam"
date: "2022-11-18"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#Load all the packages needed here
library(tidyverse)
library(readr)
library(gt)
library(tidycensus)
library(janitor)
library(lubridate)
library(modelr)
library(gridExtra)
library(car)

#setting my file path
rootdir <- ("/Users/michelle/Documents/UCSB Grad School/Courses/eds_222")
setwd(file.path(rootdir,"eds222_final_project"))
datadir <-(file.path(rootdir, "eds222_final_project","data"))

#accessing stored API key for census data
census_token <- Sys.getenv('CENSUS_KEY')
census_api_key(census_token)
```

### SGIP Data Wrangling

```{r}
#read in the SGIP data
sgip <- read_csv(file.path(datadir,"sgip_weekly_data.csv")) |> 
  janitor::clean_names()

#filter down to rebates for just residential electrochemcial storage that were not cancelled, format date and zip
sgip_res_battery <- sgip |> 
  filter(equipment_type == "Electrochemical Storage", host_customer_sector %in% c("Residential", "Single Family"), budget_classification != "Cancelled") |>
  select("city", "county", "zip", "date_received", "budget_category", "host_customer_sector") |> 
  mutate("date_received" = str_sub(date_received, 1, 8)) |> 
  mutate("date_received" = as.Date(date_received, format = "%m/%d/%y")) |>
  mutate("year_received" = year(date_received)) |>
  mutate("month_received" = month(date_received)) |> 
  mutate("zip" = ifelse(
    str_length(zip) == 10, 
    substring(zip, 1, nchar(zip)-5),
    zip))

#create sgip dataset filtered for 2020
sgip_2020 <- sgip_res_battery |> 
  filter(year_received == 2020)

#create dataframe showing count of applications in each zip code for 2020
sgip_zip_2020 <- sgip_2020 |> 
  group_by(zip) |> 
  summarize(count = n())

```

### Exploratory Data Visualization of SGIP Data

```{r}
#create a summary data frame showing applications by budget category
sgip_2020_budget <- sgip_2020 |> 
  group_by(budget_category) |> 
  summarize(count = n()) |> 
  mutate(percent = round(((count/sum(count))*100),2), percent_label = paste0(percent, "%"))

#plot percent SGIP applications by category
ggplot(sgip_2020_budget, aes(y = budget_category, x = percent)) +
  geom_bar(stat = "identity", fill = "skyblue3") +
  labs(title = "Percent of SGIP Applications in Each Budget Category (2020)", y = "Budget Category", x = "Percent of Applications") + 
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5)) +
  geom_label(label = sgip_2020_budget$percent_label, nudge_x = - 0.2)

#create df of top sgip zip codes
top_sgip_zip <- sgip_zip_2020 |> 
  slice_max(n = 10, order_by = count) |> 
  gt()

top_sgip_zip

#show how applications have changed over time
sgip_zip_all_time <- sgip_res_battery |> 
  group_by(year_received) |> 
  summarize(count = n())

ggplot(data = sgip_zip_all_time, aes(x = year_received, y = count)) +
  geom_line()
```

### Census Data Wrangling

```{r}
#access ACS data variables for 2020 year 
v20 <- load_variables(2020, "acs5", cache = TRUE)

#read in per capita income data for census tracts in CA
ca_pc_income_tract <- get_acs(geography = "tract",
                            variables = c(percapita_income = "B19301_001"),
                            state = "CA",
                            year = 2020)

#clean and format per capita income data frame
ca_pc_income_tract_clean <- ca_pc_income_tract |> 
  select(c("GEOID", "NAME", "estimate")) |> 
  rename(percapita_income = "estimate")

#read in population data for census tracts in CA
ca_pop_tract <- get_acs(geography = "tract",
                        variables = c(population = "B01003_001"),
                        state = "CA",
                        year = 2020)

#clean and format population data frame 
ca_pop_tract_clean <- ca_pop_tract |> 
  select(c("GEOID", "NAME", "estimate")) |> 
  rename(population = "estimate")

#combine per capita income and population data frames
combine_census <- cbind(ca_pc_income_tract_clean, ca_pop_tract_clean$population) |> 
  rename(population = "ca_pop_tract_clean$population", tract = "GEOID")

```

### Crosswalk File

```{r}
#read in crosswalk file to use when matching zip codes to census tracts
crosswalk <- read_csv(file.path(datadir,"ZIP_TRACT_122020.csv")) |> 
  janitor::clean_names()
```

### Combine Datasets

```{r}
#combine sgip_zip_2020 and crosswalk file by zip to get a data frame with sgip applications per census tract
#because there are multiple census tracts for one zip code, use the res_ratio to allocate what portion of the count of SGIP applications should be allocated to one census tract
sgip_zip_census <- left_join(sgip_zip_2020, crosswalk, by = "zip") |> 
  mutate("count_adjusted" = count*res_ratio) #now we have theoretical count of applications per census tract

#make sure the count_adjusted worked by filtering down to one zipcode
zip_90004 <- sgip_zip_census |> 
  filter(zip == 90004)

#combine sgip_zip_census with acs data by tract
combined_all <- left_join(sgip_zip_census, combine_census, by = "tract") |> 
  mutate("percapita_application" = count_adjusted/population) |>
  mutate("applications_per_thousand" = (count_adjusted/population)*1000) |>
  mutate("log_income" = log(percapita_income)) |> 
  mutate("income_group" = ifelse(percapita_income <= 48800, "low income", "not low income")) |> 
  select(c(-"oth_ratio", -"tot_ratio", -"bus_ratio"))#low income in CA for 2020 48,800 threshold

#check to see which census tracts are recorded as no population
zero_pop <- combined_all |> 
  filter(population == 0) |> 
  summarize(zero_pop = n())
 
#check to see which census tracts have NAs for populations
na_pop <- combined_all |> 
  filter(is.na(population)) |> 
  summarize(na_pop = n())
 
#combine zero_pop and na_pop into one data frame
missing_pop <- cbind(zero_pop, na_pop)

#check to see which census tracts have 0 for per caipta income
zero_income <- combined_all |> 
  filter(percapita_income == 0)|> 
  summarize(zero_income = n())

#check to see whcih census tracts have NA for per capita income
na_income <- combined_all |> 
  filter(is.na(percapita_income)) |> 
  summarize(na_income = n())

#combine zero_income with na-income into one data frame
missing_income <- cbind(zero_income, na_income)

#take the combined_all data frame and consolidate back to apps per zip (median of per capita income and summing population of each census tract into zip codes)
combined_by_zip <- combined_all |> 
  group_by(zip) |> 
  summarize(applications = mean(count, na.rm = TRUE), percapita_income = median(percapita_income, na.rm = TRUE), population = sum(population, na.rm = TRUE)) |> 
  filter(!is.na(percapita_income)) |> 
  mutate("applications_per_1000_people" = (applications/population)*1000, "percapita_application" = applications/population)

#top 5 applications per 1000 people zips
max_app <- combined_by_zip |> 
  filter(!is.na(percapita_income)) |> 
  slice_max(n = 5, order_by = applications_per_1000_people)

```

### Visualize Data

```{r}
#plot applications per 1000 people and per capita income
ggplot(data = combined_by_zip, aes(x = percapita_income, y = applications_per_1000_people)) +
  geom_point() +
  scale_x_continuous(name="Median Per Capita Income", labels = scales::comma) +
  labs(x = "Median Per Capita Income", y = "Application Per 1000 People", title = "SGIP Applications Per 1000 people and Median Income Per Capita (2020)") +
  theme_minimal()

```

### Testing with Simple Linear Regression Model

$$application_i =\beta_{0}+\beta_{1} \cdot income_i +\varepsilon_i$$

```{r}
#plot applications per 1000 people and per capita income (using simple linear modell)
ggplot(data = combined_by_zip, aes(x = percapita_income, y = applications_per_1000_people)) +
  geom_point() +
  geom_smooth(method = "lm", formula = y~x, se = TRUE, size = 1) +
  labs(x = "Median Per Capita Income", y = "Applications Per 1000 People", title = "SGIP Per Capita Applications and Median Income Per Capita (2020)") +
  theme_minimal()

#run linear regression
model <- lm(applications_per_1000_people ~ percapita_income, data = combined_by_zip)
summary(model)

R2 = summary(model)$r.squared
R2

b1_model <- summary(model)$coefficients["percapita_income", "Estimate"]

#what will a $10,000 increase in income per capita do to applications per thousand people based on output from linear regression?

print(paste0("On average, a $10,000 increase in income will increase applications per thousand people by ", round(10000*b1_model,2), "."))
```

### Check OLS Assumptions on Simple Linear Regression

```{r}
#get a predicted value for every observation (generate a column of predictions called pred)
#then use predictions to compute residuals (actual - prediction) -> add column called residuals
predictions <- combined_by_zip |> 
  add_predictions(model) |> 
  mutate(residuals = applications_per_1000_people - pred)

#test assumption that errors are normally distributed by creating a histogram of residuals and qq plot of residuals
residuals_hist <- ggplot(data = predictions) +
  geom_histogram(aes(residuals)) +
  labs(title = "Residuals from Simple Linear Model", x = "(apps ~ income) residuals")

income_qq <- ggplot(data = predictions,
       aes(sample = residuals)) +
  geom_qq() +
  geom_qq_line() +
  labs(title = "QQ Plot Simple Linear Regression Residuals")

grid.arrange(residuals_hist, income_qq, ncol = 2)

#test assumption that the mean of errors is zero
print(mean(predictions$residuals, na.rm = TRUE))

#test assumption that error has constant variance in x
#looking for no structured correlation
ggplot(predictions) +
  geom_point(aes(x=percapita_income, y=residuals))
```

### Check out histogram for per capita income data

```{r}
#histogram for per capita income
ggplot(data = combined_by_zip) +
  geom_histogram(aes(percapita_income), fill = "skyblue3") +
  scale_x_continuous(name="Median Per Capita Income", labels = scales::comma) +
  labs(title = "Distribution of Median Per Capita Income (2020)", y = "Count") +
  theme_minimal()

#see that the values are not normally distributed, looks like a long right tail so might try taking the log to see if it fits better

#histogram for log(per capita income)
ggplot(data = combined_by_zip) +
  geom_histogram(aes(log(percapita_income)))

```

### Run linear-log regression

$$application_i =\beta_{0}+\beta_{1} \cdot log(income_i) +\varepsilon_i$$
```{r}
#run the linear-log model
log_model <- lm(applications_per_1000_people ~ log(percapita_income), data = combined_by_zip)
summary(log_model)

#store the r squared value
R2_log_model = summary(log_model)$r.squared
R2_log_model

#store b1 coefficient from log model
b1_log_model <- summary(log_model)$coefficients["log(percapita_income)", "Estimate"]

```

#### Interpreting results when independent variable is log-transformed

```{r}
#calculate increase in applications per thousand for 1% increase in per capita income
one_percent_income <- b1_log_model/100

print(paste0("For every 1% increase in median per capita income, there is on average an increase of ", round(one_percent_income, 4), " in applications per thousand people."))

#calculate increase in applications per thousand for 10% increase in per capita income
ten_percent_income <- b1_log_model*log(1.10)

print(paste0("For every 10% increase in median per capita income, there is on average an increase of ", round(ten_percent_income, 4), " in applications per thousand people."))
```

# Check assumptions of OLS with linear-log model

```{r}
#get a predicted value for every observation (generate a column of predictions called pred)
#then use predictions to compute residuals (actual - prediction) -> add column called residuals
predictions_2 <- combined_by_zip |> 
  add_predictions(log_model) |> 
  mutate(residuals = applications_per_1000_people - pred)

#test assumption that errors are normally distributed by creating histogram and qq plot of residuals  
log_residuals_hist <- ggplot(data = predictions_2) +
  geom_histogram(aes(residuals)) +
  labs(title = "Residuals from Linear-Log Model", x = "(apps ~ log(income)) residuals")

log_income_qq <- ggplot(data = predictions_2,
       aes(sample = residuals)) +
  geom_qq() +
  geom_qq_line() +
  labs(title = "QQ Plot Linear-Log Residuals")

grid.arrange(log_residuals_hist, log_income_qq, ncol = 2)

#test assumption that the mean of errors is zero
mean(predictions_2$residuals, na.rm = TRUE)

#test assumption that error has constant variance in x
#looking for no structured correlation
ggplot(predictions_2) +
  geom_point(aes(x=percapita_income, y=residuals))

```

### Check out historgram of applications per 1000 people

```{r}
#histogram for applications per 1000 people
ggplot(data = combined_by_zip) +
  geom_histogram(aes(applications_per_1000_people), fill = "skyblue3") +
  labs(x = "SGIP Applictions Per Thousand People", y = "Count", title = "Distribution of Applications Per Thousand People (2020)") +
  theme_minimal()

#histogram for log(applications per 1000 people)
ggplot(data = combined_by_zip) +
  geom_histogram(aes(log(applications_per_1000_people)))
```

### Try Log-log model

$$log(application_i) =\beta_{0}+\beta_{1} \cdot log(income_i) +\varepsilon_i$$
```{r}
#run the log-log model
log_log_model <- lm(log(applications_per_1000_people) ~ log(percapita_income), data = combined_by_zip)
summary(log_log_model)

#store the r squared value
R2_log_log_model = summary(log_log_model)$r.squared
R2_log_log_model

#store the b1 coefficient of the log-log model
b1_log_log_model <- summary(log_log_model)$coefficients["log(percapita_income)", "Estimate"]
```

### Interpreting results when independent and dependent variable are both log transformed

```{r}
#increase in applications per thousand for 1% increase in per capita income
print(paste0("For every 1% increase in median per capita income, there is on average an increase of ", round(b1_log_log_model, 2), "% in applications per thousand people."))

#calculate increase in applications per thousand for 10% increase in per capita income
ten_percent_income_log <- (1.10^b1_log_log_model - 1) * 100

print(paste0("For every 10% increase in median per capita income, there is on average an increase of ", round(ten_percent_income_log, 2), "% in applications per thousand people."))
```

### Check OLS Assumptions with Log-Log Model

```{r}
#get a predicted value for every observation (generate a column of predictions called pred)
#then use predictions to compute residuals (actual - prediction) -> add column called residuals
predictions_3 <- combined_by_zip |> 
  add_predictions(log_log_model) |> 
  mutate(residuals = applications_per_1000_people - pred)

#test assumption that errors are normally distributed
log_log_residuals_hist <- ggplot(data = predictions_3) +
  geom_histogram(aes(residuals)) +
  labs(title = "Residuals from Log-Log Model", x = "(log(apps) ~ log(income)) residuals")

log_log_income_qq <- ggplot(data = predictions_3,
       aes(sample = residuals)) +
  geom_qq() +
  geom_qq_line() +
  labs(title = "QQ Plot Log-Log Residuals")

grid.arrange(log_log_residuals_hist, log_log_income_qq, ncol = 2)
```

### Run polynomial regression

$$application_i =\beta_{0}+\beta_{1} \cdot income_i + \beta_{2} \cdot income_i^2 + \varepsilon_i$$
```{r}
#check to see if polynomial equation better suits data
poly_model <- lm(applications_per_1000_people ~ percapita_income + I(percapita_income^2), data = combined_by_zip)

summary(poly_model)

#store all the values of the model 
R2_poly_model = summary(poly_model)$r.squared 
R2_poly_adjusted = summary(poly_model)$adj.r.squared

intercept_poly <- summary(poly_model)$coefficients["(Intercept)", "Estimate"]
b1_poly <- summary(poly_model)$coefficients["percapita_income", "Estimate"]
b2_poly <- summary(poly_model)$coefficients["I(percapita_income^2)", "Estimate"]

Predictors <- c("Intercept", "per capita income", "(per capita income)^2")
Estimate <- signif(summary(poly_model)$coefficients[,"Estimate"], digits = 3)
SE <- signif(summary(poly_model)$coefficients[,"Std. Error"], digits = 3)
p <- round(summary(poly_model)$coefficients[,"Pr(>|t|)"], 4)

poly_model_summary <- data.frame(Predictors, Estimate, SE, p)

#create a nice table to display model outputs
gt_summary <- poly_model_summary |> 
  gt() |> 
  tab_header(
    title = "Polynomial Model Output Summary"
  ) |> 
  tab_footnote(
    footnote = "Observations: 1104") |> 
  tab_footnote(
    footnote = paste("R^2/R^2 adjusted:", round(R2_poly_model,3), "/", round(R2_poly_adjusted,3))
  )

print(gt_summary)

#get a predicted value for every observation (generate a column of predictions called pred)
#then use predictions to compute residuals (actual - prediction) -> add column called residuals
predictions_4 <- combined_by_zip |> 
  add_predictions(poly_model) |> 
  mutate(residuals = applications_per_1000_people - pred)

#test assumption that errors are normally distributed
poly_residuals_hist <- ggplot(data = predictions_4) +
  geom_histogram(aes(residuals)) +
  labs(title = "Residuals from Polynomial Model", x = "(apps ~ income + income^2) residuals") +
  theme_minimal()

poly_income_qq <- ggplot(data = predictions_4,
       aes(sample = residuals)) +
  geom_qq() +
  geom_qq_line() +
  labs(title = "QQ Plot Polynomial Model Residuals") +
  theme_minimal()

grid.arrange(poly_residuals_hist, poly_income_qq, ncol = 2)

#test assumption that the mean of errors is zero
mean(predictions_4$residuals, na.rm = TRUE)

#test assumption that error has constant variance in x
#looking for no structured correlation
ggplot(predictions_4) +
  geom_point(aes(x=percapita_income, y=residuals))

#plot the polynomial line
ggplot(data = combined_by_zip, aes(x = percapita_income, y = applications_per_1000_people)) +
  geom_point() +
  geom_smooth(method = "lm", formula = y ~ x + I(x^2), size = 1) +
  scale_x_continuous(name="Median Per Capita Income", labels = scales::comma) +
  labs(x = "Median Per Capita Income", y = "Applications Per 1000 People", title = "SGIP Applications Per Thousand People and Median Income Per Capita (2020)") +
  theme_minimal()

```

### Interpretting Polynomial Regression Results 

$$application_i = -0.169 + 2.95e^{-5}\cdot income_i -1.80e^{-10}\cdot income_i^2 + \varepsilon_i$$
The effect of an increase on median per capita income on applications per thousand people depends on the baseline level of mdeian per capita income. 
What does this model predict for applications for an income of 15,000 vs. 25,000? For an income of 60,000 vs. 70,000? For an income of 100,000 vs 110,000? 
```{r}
#create a function out of the polynomial regression
app_pred_function <- function(income){
  return(intercept_poly + (b1_poly * income) + (b2_poly * (income^2)))
}

#predicted applications per 1000 people for per capita income of 15,000 vs. 25,000
pred_15000 <- app_pred_function(income = 15000)
pred_25000 <- app_pred_function(income = 25000)

diff_pred_low <- pred_25000 - pred_15000

print(paste0("When moving from a median per capita income of $15,000 to $25,000, predicted applications per thousand people increases by ", round(diff_pred_low, 2), "."))

pred_60000 <- app_pred_function(income = 60000)
pred_70000 <- app_pred_function(income = 70000)

diff_pred_mid<- pred_70000 - pred_60000

print(paste0("When moving from a median per capita income of $60,000 to $70,000, predicted applications per thousand people decreases by ", round(diff_pred_mid, 2), "."))

#predicted applications per 1000 people for per capita income of 100,000 vs. 110,000
pred_100000 <- app_pred_function(income = 100000)
pred_110000 <- app_pred_function(income = 110000)

diff_pred_high <- pred_110000- pred_100000

print(paste0("When moving from a median per capita income of $100,000 to $110,000, predicted applications per thousand people decreases by ", round(diff_pred_high, 2), "."))
```

### Run Hypothesis Testing


$$H_0: \beta_{1} = 0,  \beta_{2} = 0$$
$H_A: \beta_{j}\neq 0$ for at least one $j = 1,2$

```{r}
p_val_b1 <- summary(poly_model)$coefficients["percapita_income", "Pr(>|t|)"]

p_val_b2 <- summary(poly_model)$coefficients["I(percapita_income^2)", "Pr(>|t|)"]

#jointly test if beta1 and beta 2 are 0, test that there is no relationship at all between y and x
linearHypothesis(poly_model,c("percapita_income = 0", "I(percapita_income^2) = 0"))

```
I reject the null hypothesis that $\beta_{1}$ and $\beta_{2}$ both equal to 0 at the 0.1% significance level.

### Construct Confidence Intervals

```{r}
#create an array of confidence intervals from model predictions
conf_intervals<-predict(poly_model, interval = "confidence")

#combine confidence intervals with predictions data frame for polynomial regression model
conf_intervals_df <- cbind(predictions_4, conf_intervals)
```


